from pkgutil import get_data
import requests
from bs4 import BeautifulSoup
import typer
import csv
import json
import re
import sys
from collections import defaultdict
from collections import Counter
from playwright.sync_api import sync_playwright
import math
from typing import Annotated, Optional

app = typer.Typer()

BASE_URL2 = "https://www.ambitionbox.com/jobs/search"
url = "https://api.itjobs.pt/job"
BASE_URL = "https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav"
API_KEY = "09ad1042ebaf1704533805cd2fab64f1"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0"}


def request_api(metodo, params):
    url = "https://api.itjobs.pt/job"
    params['api_key'] = API_KEY

    if 'limit' in params:
        tamanho_pagina = 500
        total = params['limit']

        if total < tamanho_pagina:
            tamanho_pagina = total

        paginas_totais = (total // tamanho_pagina) + (1 if total % tamanho_pagina != 0 else 0)
        resultado = []

        for page in range(1, paginas_totais + 1):
            params['limit'] = tamanho_pagina
            params['page'] = page

            response = requests.get(f"{url}/{metodo}.json", headers=HEADERS, params=params)

            if response.status_code == 200:
                response_data = response.json()
                if 'results' in response_data:
                    resultado.extend(response_data['results'])
                if len(resultado) >= total:
                    break
                if len(response_data['results']) < tamanho_pagina:
                    break
            else:
                print(f"Erro ao acessar a API: {response.status_code}")
                return {}

        return {"results": resultado}

    else:
        response = requests.get(f"{url}/{metodo}.json", HEADERS={}, params=params)

        if response.status_code == 200:
            return response.json()
        else:
            print(f"Erro ao acessar a API: {response.status_code}")
            return {}

#a
def fetch_job_details(job_id: int):
    """
    Busca detalhes de um trabalho usando a API ITJobs.
    """
    url = f"https://api.itjobs.pt/job/get.json?api_key={API_KEY}&id={job_id}"
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar a API ITJobs: {e}")
        return None

def fetch_company_info(company_name: str):
    """
    Obtém informações sobre a empresa usando a página overview do AmbitionBox.
    """
    company_slug = company_name.lower().replace(" ", "-").replace(".", "-")
    url = f"https://www.ambitionbox.com/overview/{company_slug}-overview"

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            typer.echo(f"Erro ao acessar {url}. Status Code: {response.status_code}")
            return {
                "rating": "N/A",
                "description": "Informações indisponíveis",
                "benefits": "Informações indisponíveis"
            }

        soup = BeautifulSoup(response.content, 'html.parser')

        # Procurar rating
        rating_tag = soup.find("span", class_="css-1jxf684 text-primary-text font-pn-700 text-[32px]")
        rating = rating_tag.text.strip() if rating_tag else "N/A"

        # Procurar descrição
        description_tag = soup.find("div", class_="css-146c3p1 font-pn-400 text-sm text-neutral mb-2")
        description = description_tag.text.strip() if description_tag else "Informações indisponíveis"

        # Procurar benefícios
        benefits_tags = soup.find_all("div", class_="css-146c3p1 font-pn-400 text-sm text-primary-text")
        benefits = [benefit.text.strip() for benefit in benefits_tags]
        benefits = ", ".join(benefits) if benefits else "Informações indisponíveis"

        return {
            "rating": rating,
            "description": description,
            "benefits": benefits
        }

    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar o AmbitionBox: {e}")
        return {
            "rating": "N/A",
            "description": "Informações indisponíveis",
            "benefits": "Informações indisponíveis"
        }

@app.command("get")  # Adicionado este decorator
def get(job_id: int = typer.Argument(..., help="ID do trabalho a ser consultado"), export_csv: bool = False):
    """
    Busca informações de um trabalho usando o ITJobs e complementa com dados da empresa no AmbitionBox.
    
    Arguments:
        job_id: ID do trabalho a ser consultado.
        export_csv: Se verdadeiro, salva os resultados em um arquivo CSV.
    """
    job_details = fetch_job_details(job_id)
    if not job_details or 'error' in job_details:
        typer.echo(f"Trabalho com ID {job_id} não encontrado.")
        return

    company_name = job_details.get("company", {}).get("name", "N/A")
    company_info = fetch_company_info(company_name)

    result = {
        "job_id": job_id,
        "title": job_details.get("title", "N/A"),
        "company": company_name,
        "location": job_details.get("locations", [{}])[0].get("name", "N/A"),
        **company_info
    }

    typer.echo(json.dumps(result, indent=4, ensure_ascii=False))

    if export_csv:
        with open(f"job_{job_id}_details.csv", "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=result.keys())
            writer.writeheader()
            writer.writerow(result)
        typer.echo(f"Informações exportadas para job_{job_id}_details.csv")

#b
# Função para buscar informações da empresa no AmbitionBox

@app.command()
def statistics(region: str):
    """Filters job data by location and generates a CSV file."""
    params = {"limit": 1500}
    jobs_data = request_api("search", params)

    if not jobs_data or "results" not in jobs_data:
        print("Error: Unable to retrieve job data.")
        return

    filtered_jobs = []

    for job in jobs_data["results"]:
        for location in job.get("locations", []):
            area = location["name"]

            if region.lower() in area.lower():
                filtered_jobs.append({
                    "title": job.get("title", "Not specified"),
                    "type": ", ".join(t["name"] for t in job.get("types", [])) or "Not specified",
                    "area": area
                })

    if not filtered_jobs:
        print(f"No jobs found for region: {region}")
        return

    file_name = f"filtered_jobs_{region.replace(' ', '_').lower()}.csv"
    with open(file_name, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=["title", "type", "area"])
        writer.writeheader()
        writer.writerows(filtered_jobs)

    print(f"CSV file '{file_name}' created successfully.")

if __name__ == "__main__":
    app()


def fetch_job_details(job_id: int):
    """
    Busca detalhes de um trabalho usando a API ITJobs.
    """
    url = f"https://api.itjobs.pt/job/get.json?api_key={API_KEY}&id={job_id}"
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar a API ITJobs: {e}")
        return None

def fetch_indeed_company_info(company_name: str):
    """
    Obtém informações sobre a empresa usando a página de overview do Indeed.
    """
    company_slug = company_name.lower().replace(" ", "-").replace(".", "-")
    url = f"https://www.indeed.com/cmp/{company_slug}"

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            typer.echo(f"Erro ao acessar {url}. Status Code: {response.status_code}")
            return {
                "rating": "N/A",
                "review_count": "N/A",
                "benefits": "Informações indisponíveis"
            }

        soup = BeautifulSoup(response.content, 'html.parser')

        # Procurar rating
        rating_tag = soup.find("meta", {"name": "ratingValue"})
        rating = rating_tag["content"] if rating_tag else "N/A"

        # Procurar número de reviews
        review_tag = soup.find("meta", {"name": "reviewCount"})
        review_count = review_tag["content"] if review_tag else "N/A"

        # Procurar benefícios
        benefits_section = soup.find("div", class_="css-1qibq6a")
        benefits = benefits_section.text.strip() if benefits_section else "Informações indisponíveis"

        return {
            "rating": rating,
            "review_count": review_count,
            "benefits": benefits
        }

    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar o Indeed: {e}")
        return {
            "rating": "N/A",
            "review_count": "N/A",
            "benefits": "Informações indisponíveis"
        }

@app.command("get2")  # Adicionado este decorator
def get2(job_id: int = typer.Argument(..., help="ID do trabalho a ser consultado"), export_csv: bool = False):
    """
    Busca informações de um trabalho usando o ITJobs e complementa com dados da empresa no Indeed.
    
    Arguments:
        job_id: ID do trabalho a ser consultado.
        export_csv: Se verdadeiro, salva os resultados em um arquivo CSV.
    """
    job_details = fetch_job_details(job_id)
    if not job_details or 'error' in job_details:
        typer.echo(f"Trabalho com ID {job_id} não encontrado.")
        return

    company_name = job_details.get("company", {}).get("name", "N/A")
    company_info = fetch_indeed_company_info(company_name)

    result = {
        "job_id": job_id,
        "title": job_details.get("title", "N/A"),
        "company": company_name,
        "location": job_details.get("locations", [{}])[0].get("name", "N/A"),
        **company_info
    }

    typer.echo(json.dumps(result, indent=4, ensure_ascii=False))

    if export_csv:
        with open(f"job_{job_id}_indeed_details.csv", "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=result.keys())
            writer.writeheader()
            writer.writerow(result)
        typer.echo(f"Informações exportadas para job_{job_id}_indeed_details.csv")
