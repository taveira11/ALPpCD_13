import requests
from bs4 import BeautifulSoup
import typer
import csv
import json
import re
import sys
from collections import defaultdict
from collections import Counter
from playwright.sync_api import sync_playwright
import math


app = typer.Typer()

BASE_URL2 = "https://www.ambitionbox.com/jobs/search"
url = "https://api.itjobs.pt/job"
BASE_URL = "https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav"
API_KEY = "09ad1042ebaf1704533805cd2fab64f1"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0"}


def request_website(BASE_URL2):
    try:
        response = requests.get(BASE_URL2, headers=HEADERS)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "lxml")
            return soup
        else:
            print(f"Erro {response.status_code}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"Erro na requisição: {e}")
        return None


def request_api(metodo, params):
    url = "https://api.itjobs.pt/job"
    params['api_key'] = API_KEY

    if 'limit' in params:
        tamanho_pagina = 500
        total = params['limit']

        if total < tamanho_pagina:
            tamanho_pagina = total

        paginas_totais = (total // tamanho_pagina) + (1 if total % tamanho_pagina != 0 else 0)
        resultado = []

        for page in range(1, paginas_totais + 1):
            params['limit'] = tamanho_pagina
            params['page'] = page

            response = requests.get(f"{url}/{metodo}.json", headers=HEADERS, params=params)

            if response.status_code == 200:
                response_data = response.json()
                if 'results' in response_data:
                    resultado.extend(response_data['results'])
                if len(resultado) >= total:
                    break
                if len(response_data['results']) < tamanho_pagina:
                    break
            else:
                print(f"Erro ao acessar a API: {response.status_code}")
                return {}

        return {"results": resultado}

    else:
        response = requests.get(f"{url}/{metodo}.json", HEADERS={}, params=params)

        if response.status_code == 200:
            return response.json()
        else:
            print(f"Erro ao acessar a API: {response.status_code}")
            return {}


def fetch_job_details(job_id: int):
    """
    Busca detalhes de um trabalho usando a API ITJobs.
    """
    url = f"https://api.itjobs.pt/job/get.json?api_key={API_KEY}&id={job_id}"
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar a API ITJobs: {e}")
        return None

def fetch_company_info(company_name: str):
    """
    Obtém informações sobre a empresa usando a página overview do AmbitionBox.
    """
    company_slug = company_name.lower().replace(" ", "-").replace(".", "-")
    url = f"https://www.ambitionbox.com/overview/{company_slug}-overview"

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            typer.echo(f"Erro ao acessar {url}. Status Code: {response.status_code}")
            return {
                "rating": "N/A",
                "description": "Informações indisponíveis",
                "benefits": "Informações indisponíveis"
            }

        soup = BeautifulSoup(response.content, 'html.parser')

        # Procurar rating
        rating_tag = soup.find("span", class_="css-1jxf684 text-primary-text font-pn-700 text-[32px]")
        rating = rating_tag.text.strip() if rating_tag else "N/A"

        # Procurar descrição
        description_tag = soup.find("div", class_="css-146c3p1 font-pn-400 text-sm text-neutral mb-2")
        description = description_tag.text.strip() if description_tag else "Informações indisponíveis"

        # Procurar benefícios
        benefits_tags = soup.find_all("div", class_="css-146c3p1 font-pn-400 text-sm text-primary-text")
        benefits = [benefit.text.strip() for benefit in benefits_tags]
        benefits = ", ".join(benefits) if benefits else "Informações indisponíveis"

        return {
            "rating": rating,
            "description": description,
            "benefits": benefits
        }

    except requests.RequestException as e:
        typer.echo(f"Erro ao acessar o AmbitionBox: {e}")
        return {
            "rating": "N/A",
            "description": "Informações indisponíveis",
            "benefits": "Informações indisponíveis"
        }

@app.command("get")  # Adicionado este decorator
def get(job_id: int = typer.Argument(..., help="ID do trabalho a ser consultado"), export_csv: bool = False):
    """
    Busca informações de um trabalho usando o ITJobs e complementa com dados da empresa no AmbitionBox.
    
    Arguments:
        job_id: ID do trabalho a ser consultado.
        export_csv: Se verdadeiro, salva os resultados em um arquivo CSV.
    """
    job_details = fetch_job_details(job_id)
    if not job_details or 'error' in job_details:
        typer.echo(f"Trabalho com ID {job_id} não encontrado.")
        return

    company_name = job_details.get("company", {}).get("name", "N/A")
    company_info = fetch_company_info(company_name)

    result = {
        "job_id": job_id,
        "title": job_details.get("title", "N/A"),
        "company": company_name,
        "location": job_details.get("locations", [{}])[0].get("name", "N/A"),
        **company_info
    }

    typer.echo(json.dumps(result, indent=4, ensure_ascii=False))

    if export_csv:
        with open(f"job_{job_id}_details.csv", "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=result.keys())
            writer.writeheader()
            writer.writerow(result)
        typer.echo(f"Informações exportadas para job_{job_id}_details.csv")


# Função para buscar informações da empresa no AmbitionBox
def fetch_page_content(page_number):
    """Faz uma requisição HTTP e retorna o conteúdo da página."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0 Safari/537.36"
    }
    params = {"page": page_number}
    response = requests.get(BASE_URL, headers=headers, params=params)
    if response.status_code == 200:
        return response.content
    else:
        print(f"Erro ao acessar a página {page_number}: {response.status_code}")
        return None

def parse_job_links(page_content):
    """Extrai os links das vagas disponíveis na página."""
    soup = BeautifulSoup(page_content, "html.parser")
    job_links = []
    for meta_tag in soup.find_all("meta", {"itemprop": "url"}):
        if meta_tag.get("content"):
            job_links.append(meta_tag["content"])
    return job_links

def extract_job_details(job_url):
    """Coleta os detalhes de uma vaga específica."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0 Safari/537.36"
    }
    response = requests.get(job_url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        try:
            title = soup.select_one("h1.desig.bold-title").text.strip()
            location = soup.select_one("div.entity.loc p.body-small-l").text.strip()
            vacancies = soup.select_one("div.entity.vacancy p.body-small-l").text.strip()
            vacancy_count = int(vacancies.split()[0])
            return title, location, vacancy_count
        except AttributeError:
            print(f"Detalhes não encontrados para {job_url}")
            return None
    else:
        print(f"Erro ao acessar {job_url}: {response.status_code}")
        return None

def consolidate_job_data(job_data):
    """Consolida as informações das vagas em um formato agrupado."""
    summary = {}
    for location, title, vacancies in job_data:
        key = (location, title)
        if key in summary:
            summary[key] += vacancies
        else:
            summary[key] = vacancies
    return summary

def save_to_csv(data, filename="job_statistics.csv"):
    """Salva os dados consolidados em um arquivo CSV."""
    with open(filename, mode="w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Zona", "Tipo de Trabalho", "Nº de Vagas"])
        for (location, title), vacancies in data.items():
            writer.writerow([location, title, vacancies])
    print(f"Arquivo {filename} gerado com sucesso!")

@app.command()
def generate_statistics(
    display: str = typer.Option(
        "both", help="Choose 'title' for job title, 'type' for job type, or 'both' for both."
    ),
    region: str = typer.Option(
        None, help="Specify the region to filter job listings (optional)."
    )
):
    """Generates a CSV file with job count statistics by area, job title, or job type based on user choice."""
    params = {"limit": 1500}
    jobs_data = request_api("search", params)

    if not jobs_data or "results" not in jobs_data:
        typer.echo("Failed to retrieve job data.")
        return

    # Dictionary to group jobs by area
    jobs_by_area = {}

    for job in jobs_data["results"]:
        # Extract location
        for location in job.get("locations", []):
            area = location["name"]

            # Skip areas not matching the specified region
            if region and region.lower() not in area.lower():
                continue

            # Extract job title
            job_title = job.get("title", "Not specified")

            # Extract job type
            job_type = ", ".join(t["name"] for t in job.get("types", [])) or "Not specified"

            if area not in jobs_by_area:
                jobs_by_area[area] = {}

            # Grouping based on user choice
            if display == "title":
                key = job_title
            elif display == "type":
                key = job_type
            else:  # If "both" is chosen
                key = (job_title, job_type)

            if key not in jobs_by_area[area]:
                jobs_by_area[area][key] = 0

            jobs_by_area[area][key] += 1

    # Writing to CSV
    with open("job_statistics.csv", "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)

        # Conditional header
        if display == "title":
            writer.writerow(["Area", "Job Title", "Job Count"])
        elif display == "type":
            writer.writerow(["Area", "Job Type", "Job Count"])
        else:  # "both"
            writer.writerow(["Area", "Job Title", "Job Type", "Job Count"])

        # Writing rows
        for area, job_info in jobs_by_area.items():
            for key, count in job_info.items():
                if display == "both":
                    job_title, job_type = key
                    writer.writerow([area, job_title, job_type, count])
                else:
                    writer.writerow([area, key, count])

    print("File 'job_statistics.csv' created successfully.")

if __name__ == "__main__":
    app()




@app.command()
def salary(job_id: int, export_csv: bool = False):
    """
    Extrai o salário oferecido para um determinado job id.
    """
    try:
        print(f"A procurar as informações de salário para o job id: {job_id}...")

        headers = {
            "User-Agent": "jobs-cli"  
        }

        # Faz a requisição para a API
        response = requests.get(
            BASE_URL2.format(job_id=job_id),
            headers=headers,
            params={"api_key": API_KEY}
        )

        # Verifica se a requisição foi bem-sucedida
        response.raise_for_status()
        data = response.json()

        job = data.get("results", [])[0]

        wage = job.get("wage")
        body = job.get("body", "")

        salary_data = {
            "salary": wage if wage else "Não especificado"
        }

        # Expressão regular para buscar possíveis menções a salários no corpo da descrição
        salary_matches = re.findall(
            r'(\d{1,3}(?:\.\d{3})*,?\d{0,2}\s*(?:EUR|€|euro|euros|k))', body, re.IGNORECASE
        )

        if salary_matches:
            salary_data["salary_matches"] = ", ".join(salary_matches)

        if export_csv:
            export_to_csv([salary_data], "salary_jobs.csv")

        print(json.dumps(salary_data, indent=2, ensure_ascii=False))

    except requests.RequestException as e:
        print(f"Erro ao acessar a API: {e}")
    except Exception as e:
        print(f"Erro inesperado: {e}")



def generate_statistics(
    display: str = typer.Option(
        "both", help="Choose 'title' for job title, 'type' for job type, or 'both' for both."
    ),
    region: str = typer.Option(
        None, help="Specify the region to filter job listings (optional)."
    )
):
    """Generates a CSV file with job count statistics by area, job title, or job type based on user choice."""
    params = {"limit": 1500}
    jobs_data = request_api("search", params)

    if not jobs_data or "results" not in jobs_data:
        typer.echo("Failed to retrieve job data.")
        return

    # Dictionary to group jobs by area
    jobs_by_area = {}

    for job in jobs_data["results"]:
        # Extract location
        for location in job.get("locations", []):
            area = location["name"]

            # Skip areas not matching the specified region
            if region and region.lower() not in area.lower():
                continue

            # Extract job title
            job_title = job.get("title", "Not specified")

            # Extract job type
            job_type = ", ".join(t["name"] for t in job.get("types", [])) or "Not specified"

            if area not in jobs_by_area:
                jobs_by_area[area] = {}

            # Grouping based on user choice
            if display == "title":
                key = job_title
            elif display == "type":
                key = job_type
            else:  # If "both" is chosen
                key = (job_title, job_type)

            if key not in jobs_by_area[area]:
                jobs_by_area[area][key] = 0

            jobs_by_area[area][key] += 1

    # Writing to CSV
    with open("job_statistics.csv", "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)

        # Conditional header
        if display == "title":
            writer.writerow(["Area", "Job Title", "Job Count"])
        elif display == "type":
            writer.writerow(["Area", "Job Type", "Job Count"])
        else:  # "both"
            writer.writerow(["Area", "Job Title", "Job Type", "Job Count"])

        # Writing rows
        for area, job_info in jobs_by_area.items():
            for key, count in job_info.items():
                if display == "both":
                    job_title, job_type = key
                    writer.writerow([area, job_title, job_type, count])
                else:
                    writer.writerow([area, key, count])

    print("File 'job_statistics.csv' created successfully.")









 

def export_to_csv(jobs, filename):
    """Exporta a lista de jobs para um arquivo CSV."""
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        # Adicionar cabeçalho baseado nos campos do job
        if filename == "salary_jobs.csv":
            writer.writerow(["job_id", "title", "company", "location", "description", "salary", "salary_matches"])
        else:
            writer.writerow(["titulo", "empresa", "descrição", "data de publicação", "salário", "localização"])

        for job in jobs:
            writer.writerow([
                job.get("job_id", "N/A"),
                job.get("title", "N/A"),
                job.get("company", "N/A"),
                job.get("location", "N/A"),
                job.get("description", "N/A"),
                job.get("salary", "N/A"),
                job.get("salary_matches", "N/A")  # Para salary_jobs.csv
            ])
        typer.echo(f"Dados exportados para {filename}")

if __name__ == "__main__":
    app()
